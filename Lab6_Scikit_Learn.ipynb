{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of JSC270_Lab6_PartII_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsAiyaya/JSC270_Data_Science_1/blob/main/Lab6_Scikit_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHssrOTFpC5V"
      },
      "source": [
        "# __JSC270 Lab 6 Part II: Intro to Scikit-Learn__\n",
        "\n",
        "- __Note__: This part of the tutorial will be a bit less interactive. The goal here is to showcase some of the techniques you'll need, with an emphasis on fitting models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SsPi8OpCO8"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. What is Scikit-learn?\n",
        "2. Datasets\n",
        "3. Some common preprocessing techniques\n",
        "- Scaling\n",
        "- One-hot Encoding\n",
        "- Imputation\n",
        "4. The Universality of SKLearn Design\n",
        "5. Linear Regression (Fit, Fit-transform, Predict)\n",
        "6. Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXTKa4VbqyMP"
      },
      "source": [
        "# __1. What is Scikit-Learn?__\n",
        "\n",
        "<br>\n",
        "\n",
        "- Scikit-learn is the largest and likely the most popular machine learning/ data analysis API (written for python)\n",
        "-  It is written at a high level using packages we have or will cover in this class (numpy, scipy, matplotlib)\n",
        "- It contains virtually all standard models and processes a data scientist will want to use (and many others)\n",
        "- Because it is written at such a high level, most of the details are removed, allowing users to analyze data with very little code\n",
        "- Like numpy and other main python packages, it has excellent documentation. I encourage you to visit the website:\n",
        "[https://scikit-learn.org/stable/index.html]\n",
        "\n",
        "- Though it contains libraries for almost every part of the data science pipeline (collection, cleaning, exploratory analysis, modelling, pipelining, deployment, etc.), today we'll mostly focus on fitting models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm3Hca177smt"
      },
      "source": [
        "<br>\n",
        "\n",
        "# __2. Datasets__\n",
        "\n",
        "Today we're going to revisit the __California Housing Dataset__ which we saw a few weeks back.  The data contains a number of characteristics about the houses in California.  This dataset was derived from the 1990 U.S. census.\n",
        "\n",
        "We'll run through the details of scikit learn with this dataset.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import our dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Remind yourself of the details of the dataset \n",
        "print(fetch_california_housing(as_frame = True).DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8PspIsYwti4",
        "outputId": "cf2cda5c-8fd6-4133-9683-8c33924cd032"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "An household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surpinsingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading in the dataset. Scikit learn actually has a number of built-in datasets that are great for practicing analysis skills.\n",
        "\n",
        "Some things to note:\n",
        "- Scikit-learn loads the data in as numpy arrays\n",
        "- Unfortunately this means that you lose the metadata that would come with an external dataset"
      ],
      "metadata": {
        "id": "9S9t3bLmhlXJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAlaYL4H_U7U",
        "outputId": "79a8c860-c0a2-4888-aeca-117fc3bca77f"
      },
      "source": [
        "import numpy as np\n",
        "# Avoid displaying floats with scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# We load in the features and target separately\n",
        "X,y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Slight reshaping of target like before\n",
        "n = X.shape[0] # number of observations\n",
        "d = X.shape[1] # number of features\n",
        "y = np.reshape(y, newshape=(n,1))\n",
        "\n",
        "print('shape of features: ',X.shape)\n",
        "print('shape of target: ', y.shape)\n",
        "print(type(X))\n",
        "\n",
        "print('This matrix contains data of type: ',X.dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of features:  (20640, 8)\n",
            "shape of target:  (20640, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "This matrix contains data of type:  float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvTVd3W3DBpS"
      },
      "source": [
        "Generally, your matrix of covariates should be $n \\times p$, where n is the number of observations, and $p$ is the number of features. Thus each column is a different feature, and each row is an observation. Similarly your target vector should be $n \\times 1$. This is what we have here (which is always a good sign). \n",
        "- To help with the missing metadeta, I've created a separate dictionary that contains the information we'll need to interpret features.\n",
        "- All this information can be found in the [scikit-learn documentation](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) for this dataset.\n",
        "- The order of keys corresponds exactly to the order of the columns in the feature matrix (with the exception of the 14th feature, which is actually the target: median house price)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA-h5978I-Yq",
        "outputId": "0bdd1c35-4779-4ead-9c3f-bad17c8506bb"
      },
      "source": [
        "# Store variable info to help with interpretation later\n",
        "variable_info = {0: 'median income in block group',\n",
        "                 1: 'median house age in block group',\n",
        "                 2: 'average number of rooms per household',\n",
        "                 3: 'average number of bedrooms per household',\n",
        "                 4: 'block group population',\n",
        "                 5: 'average number of household members',\n",
        "                 6: 'block group latitude',\n",
        "                 7: 'block group longitude'}\n",
        "print(variable_info[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "median income in block group\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P0jb2CSNcZM"
      },
      "source": [
        "Let's look at the first five observations of the first 5 features. Recall how to slice a numpy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XTIVTcRNmvw",
        "outputId": "a5168ad9-7c1f-4825-853b-da8fd47945e1"
      },
      "source": [
        "print('First 5 observations of the first 5 features:\\n',X[:5,:5])\n",
        "print('These are the first five variables:\\n')\n",
        "for i in range(5):\n",
        "  print(i+1, variable_info[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 observations of the first 5 features:\n",
            " [[   8.3252       41.            6.98412698    1.02380952  322.        ]\n",
            " [   8.3014       21.            6.23813708    0.97188049 2401.        ]\n",
            " [   7.2574       52.            8.28813559    1.07344633  496.        ]\n",
            " [   5.6431       52.            5.8173516     1.07305936  558.        ]\n",
            " [   3.8462       52.            6.28185328    1.08108108  565.        ]]\n",
            "These are the first five variables:\n",
            "\n",
            "1 median income in block group\n",
            "2 median house age in block group\n",
            "3 average number of rooms per household\n",
            "4 average number of bedrooms per household\n",
            "5 block group population\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "__Q: Are all these features actually numeric? If no, which ones are categorical?__ "
      ],
      "metadata": {
        "id": "HiIuCNllkGyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add answer below \n",
        "X.dtype\n",
        "# All the variables are numeric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuJxJHfKkGRr",
        "outputId": "cd5dfaed-3dac-4b62-ae47-c0cb33e32469"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57_HWLDaW0nE"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Because this is a numpy array, all of our `numpy` functions from earlier still apply\n",
        "- Let's get some summary statistics for each of our last 5 features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBtfPFjHXe6T",
        "outputId": "810b050a-8a9c-4e3f-ad0c-48cad6a07726"
      },
      "source": [
        "print('Maximum values:\\n',np.max(X[:,-5:], axis = 0))\n",
        "print('Minimum values:\\n',np.min(X[:,-5:], axis=0))\n",
        "print('Median values:\\n',np.median(X[:,-5:], axis = 0))\n",
        "print('Standard Deviations:\\n', np.std(X[:,-5:], axis=0))\n",
        "\n",
        "# Careful to specify the correct axis here\n",
        "\n",
        "# Print the variables we're looking at:\n",
        "for i in range(3, 8):\n",
        "  print(i+1, variable_info[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum values:\n",
            " [   34.06666667 35682.          1243.33333333    41.95\n",
            "  -114.31      ]\n",
            "Minimum values:\n",
            " [   0.33333333    3.            0.69230769   32.54       -124.35      ]\n",
            "Median values:\n",
            " [   1.04878049 1166.            2.81811565   34.26       -118.49      ]\n",
            "Standard Deviations:\n",
            " [   0.47389938 1132.43468776   10.38579796    2.13590065    2.00348319]\n",
            "4 average number of bedrooms per household\n",
            "5 block group population\n",
            "6 average number of household members\n",
            "7 block group latitude\n",
            "8 block group longitude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCgo-j7KecWz"
      },
      "source": [
        "<br>\n",
        "\n",
        "These numbers seem reasonable.\n",
        "\n",
        "__Warning: We should do a lot more preprocessing than you'll see in this notebook (and probably additional feature engineering too). Demonstrating the design of Sci-kit learn and showing you how to fit models is the focus here, but in reality exploring and cleaning the data will take up most of your time. This dataset is fairly clean, with no missing values, but I will touch on some cleaning techniques shortly.__\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __3. Some Common Preprocessing Techniques__\n",
        "- Many of these techniques do not apply to our dataset, but I'll generate additional random covariates (that will not be used for modelling) to show you how they work.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Categorical encoding\n",
        "\n",
        "- Although the data we loaded in earlier has some categorical variables, they are all binary (0 or 1). If we did have a categorical variable with, say, 5 categories, there are several ways we might choose to represent them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnJrdFF5hLEU",
        "outputId": "89dbd046-c1d7-4b43-f346-d88146c7a095"
      },
      "source": [
        "# Generate a random covariate\n",
        "# Recall that n is the number of observations\n",
        "cat_variable = np.random.choice(['A','B','C','D','E'], size = n)\n",
        "cat_variable = np.reshape(cat_variable,(n,1))\n",
        "\n",
        "# Show the first 10 observations\n",
        "print(cat_variable[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['B']\n",
            " ['A']\n",
            " ['B']\n",
            " ['E']\n",
            " ['A']\n",
            " ['C']\n",
            " ['B']\n",
            " ['E']\n",
            " ['A']\n",
            " ['E']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXK25wrajX1D"
      },
      "source": [
        "Keeping our categories stored as strings might not be very helpful when we want to train a model. Virtually every model we'll use requires the input to be a number (even if that number represents a category). We can use Sci-kit learn's __ordinal encoder__ to convert these letters to numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSXhmueRjwcJ",
        "outputId": "45ec4b78-c156-4aca-f62b-707b96ab18aa"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Instantiate the ordinal encoder object using the OrdinalEncoder class\n",
        "# You could provide specific encoding numbers as a hyperparameter if you want to\n",
        "ord_encoder = OrdinalEncoder()\n",
        "\n",
        "# fit the encoder to our data\n",
        "cat_var_ord = ord_encoder.fit_transform(cat_variable)\n",
        "\n",
        "# Show the first 10 observations\n",
        "print(cat_var_ord[:10])\n",
        "\n",
        "# What if I want to see how the old categories are represented here?\n",
        "print('\\nThese are the categories:\\n',ord_encoder.categories_)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [4.]\n",
            " [0.]\n",
            " [2.]\n",
            " [1.]\n",
            " [4.]\n",
            " [0.]\n",
            " [4.]]\n",
            "\n",
            "These are the categories:\n",
            " [array(['A', 'B', 'C', 'D', 'E'], dtype='<U1')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxH-GF1Yhpde"
      },
      "source": [
        "Representing a variable with ordinal encoding can be misleading if the order of the categories does not matter. Instead, we might want to make each category its own feature. This is called __one-hot encoding__, and under this representation, each of our five categories becomes a dummy variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ker2ga6Rh_YX",
        "outputId": "7955ff28-7236-4309-b68c-9156d4d0dd2a"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Again, instantiate the encoder object\n",
        "ohot_encoder = OneHotEncoder()\n",
        "\n",
        "# Fit the encoder to our data\n",
        "cat_var_onehot = ohot_encoder.fit_transform(cat_variable)\n",
        "\n",
        "# Print the new encoding\n",
        "print('One-hot encoded:\\n',cat_var_onehot[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoded:\n",
            "   (0, 1)\t1.0\n",
            "  (1, 0)\t1.0\n",
            "  (2, 1)\t1.0\n",
            "  (3, 4)\t1.0\n",
            "  (4, 0)\t1.0\n",
            "  (5, 2)\t1.0\n",
            "  (6, 1)\t1.0\n",
            "  (7, 4)\t1.0\n",
            "  (8, 0)\t1.0\n",
            "  (9, 4)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C5G-jGdm-em"
      },
      "source": [
        "The default output of `OneHotEncoder` is actually a scipy matrix, which prints a little differently (it's designed for very large, sparse datasets). To convert back to our usual array, we can use the `toarray()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klY7QLgdm_2X",
        "outputId": "a484fff9-ee3a-463c-9aac-f74c16c941fe"
      },
      "source": [
        "print(cat_var_onehot.toarray()[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnqFY3L4ngCf"
      },
      "source": [
        "Notice that 1 feature has been turned into 5.\n",
        "\n",
        "<br>\n",
        "\n",
        "There are other encoders in the preproccessing library, but these two are the most common.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Scaling and Standardizing\n",
        "\n",
        "- Another common preprocessing step is standardizing features. Some models (e.g. neural nets, penalized regression, or distance-based algos like KNN) perform much better with standardized inputs.\n",
        "\n",
        "- Standardization doesn't make much sense on categorical variables, so i'll generate 2 new features (which again will not be used for modelling) to demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s01PAogbpItn",
        "outputId": "e131e7f3-ae0d-4c12-a316-d5f01dc46929"
      },
      "source": [
        "# Generate the first feature\n",
        "x1 = np.random.uniform(-50,0,n)\n",
        "x2 = np.random.uniform(0,100,n)\n",
        "\n",
        "# Reshape\n",
        "x1, x2 = np.reshape(x1, (n,1)), np.reshape(x2, (n,1))\n",
        "\n",
        "# Combine into a single feature matrix\n",
        "X_sim = np.concatenate([x1,x2], axis=1)\n",
        "\n",
        "# Print first 10 observations of both features\n",
        "print('Simulated features:\\n',X_sim[:10,:])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated features:\n",
            " [[-26.82691525  54.73515894]\n",
            " [-38.30502225  27.70951576]\n",
            " [-32.24168577   5.00383666]\n",
            " [-12.72728812  19.44034635]\n",
            " [-28.9844016    7.75186594]\n",
            " [-10.6506309    0.96698987]\n",
            " [ -2.4259646   73.98916036]\n",
            " [-46.90202282  66.47843828]\n",
            " [-20.87640015  99.27930604]\n",
            " [-30.97279476  33.50440325]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrF-EaF2qAYk"
      },
      "source": [
        "We can see that the two features are very different. We can use SKLearn to scale them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1vOjjKPqJCZ",
        "outputId": "de7d05e3-8eed-466c-cc84-3ece9b2b0815"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Instantiate the transformer object\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# Fit to data (subtract mean and divide by SD for each column)\n",
        "scaled_X = std_scaler.fit_transform(X_sim)\n",
        "\n",
        "# Print new features\n",
        "print('New standardized features',scaled_X[:10,:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New standardized features [[-0.13444784  0.17395167]\n",
            " [-0.92842677 -0.75755242]\n",
            " [-0.50900557 -1.54015851]\n",
            " [ 0.84087038 -1.04256932]\n",
            " [-0.28368836 -1.44544104]\n",
            " [ 0.98451968 -1.67929819]\n",
            " [ 1.55344727  0.83758736]\n",
            " [-1.52310996  0.57871217]\n",
            " [ 0.27716913  1.70927333]\n",
            " [-0.42123214 -0.55781762]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wUQhd__riMN"
      },
      "source": [
        "We can see that the features have been standardized. There are other scalers (ie min-max scaling is common for discrete features), but we will not look at them here.\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfwrcx_H8wrg"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Imputation\n",
        "- One final preprocessing step is handling missing data. Since we have no missing data in the features above, let's generate some"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjbMGIj_9Ejz",
        "outputId": "27fdd8af-da39-470a-ded2-6457189ec79d"
      },
      "source": [
        "# Generate the domain of our feature, which can include a missing value\n",
        "choices_of_x = list(range(51)) + [np.nan]\n",
        "\n",
        "np.random.seed(10)\n",
        "x_with_na = np.random.choice(choices_of_x, size=n)\n",
        "x_with_na = np.reshape(x_with_na,(n,1))\n",
        "\n",
        "# Print some values of our feature\n",
        "print('Feature with missing values:\\n',x_with_na[:20])\n",
        "\n",
        "# How many missing values are there?\n",
        "# Generate an identical vector containing TRUE(1) where missing, FALSE(0) where there is data\n",
        "# Then just sum that vector to get the number of missing values\n",
        "print('\\nOut of {} observations, we have {} missing values'.format(n, np.sum(np.isnan(x_with_na)) ) )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature with missing values:\n",
            " [[ 9.]\n",
            " [36.]\n",
            " [15.]\n",
            " [ 0.]\n",
            " [49.]\n",
            " [28.]\n",
            " [25.]\n",
            " [29.]\n",
            " [48.]\n",
            " [29.]\n",
            " [49.]\n",
            " [ 8.]\n",
            " [ 9.]\n",
            " [ 0.]\n",
            " [42.]\n",
            " [40.]\n",
            " [36.]\n",
            " [nan]\n",
            " [16.]\n",
            " [36.]]\n",
            "\n",
            "Out of 20640 observations, we have 399 missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3foygSNp-uaS"
      },
      "source": [
        "We'v talked about missing data in lecture and lab.  One option to handle missing values is to impute them with reasonable values.\n",
        "\n",
        "- For demonstration purposes, let's replace the `nan` values with the median of the column\n",
        "- This is not the only choice, but it is a popular one "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E56qynU_AcNB",
        "outputId": "faf7340e-1cf4-4a1d-fbcc-0c1ac240ddcf"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Instantiate the imputer transformer\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Fit the imputation to the dataset\n",
        "# Notice that since I already specified median earlier, I don't have to do so here\n",
        "X_imputed = median_imputer.fit_transform(x_with_na)\n",
        "\n",
        "# Print imputed data\n",
        "print('Feature with imputed missing values:\\n',X_imputed[:20])\n",
        "\n",
        "# We can also access the median that our imputer used:\n",
        "print('Medians used to replace missing values:\\n',median_imputer.statistics_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature with imputed missing values:\n",
            " [[ 9.]\n",
            " [36.]\n",
            " [15.]\n",
            " [ 0.]\n",
            " [49.]\n",
            " [28.]\n",
            " [25.]\n",
            " [29.]\n",
            " [48.]\n",
            " [29.]\n",
            " [49.]\n",
            " [ 8.]\n",
            " [ 9.]\n",
            " [ 0.]\n",
            " [42.]\n",
            " [40.]\n",
            " [36.]\n",
            " [25.]\n",
            " [16.]\n",
            " [36.]]\n",
            "Medians used to replace missing values:\n",
            " [25.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ2GNxr1Bs-7"
      },
      "source": [
        "We can see that replacing missing values is very simple. Note that if we had more than one feature (p > 1), this imputer object would have computed the median for each feature, and replaced missing observations with the median belonging to the appropriate column.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## __QUESTIONS (About anything so far)?__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-S5ICtTasY"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __4. The Universality of Scikit-Learn Design__\n",
        "\n",
        "- Many of these classes are very similar (You're probably noticing a general pattern)\n",
        "\n",
        "- This is because the Scikit-Learn API is designed for consistency and simplicity\n",
        "\n",
        "- Scikit-Learn has three general classes of objects:\n",
        "\n",
        "    1. __Estimators__: Any object that estimates parameters based on data is an _estimator_ (for example, most models estimate at least one coefficient). Estimation is always done with the `.fit()` method, which always has only one required argument: the data. (For some supervised learning algorithms, the data may be passed as two arguments X,y). \n",
        "\n",
        "    2. __Transformers__: Some estimators also transform a dataset. All transformations are done with the `.transform()` method. Sometimes we can combine estimation and transformation using a `fit_transform()` method. The transformation generally relies on estimated/learned parameters (e.g our imputation estimated the medians before applying them). \n",
        "\n",
        "    3. __Predictors__: Given a dataset, some classes called _predictors_ can make predictions on new data. This is always done with the `predict()` method, which takes new data and returns predicted values. Every predictor also has a `score()` method, which evaluates the quality of predictions.\n",
        "\n",
        "Some other useful information:\n",
        "- Hyperparameters (ie things we choose as data analysts) are always available as variables, even after we instantiate the estimator (for example, our 'median' imputer strategy)\n",
        "- Scikit-Learn almost always uses well-informed, sensible default values for optional arguments\n",
        "- Datasets are always represented as NumPy arrays or SciPy sparse matrices, and hyperparameters are always regular python strings or numbers (no homemade classes or datatypes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uckmo-kvTbli"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __5. Linear Regression__\n",
        "- You've all been waiting patiently so far\n",
        "- Let's actually fit a model to our data\n",
        "- Since our goal will be to predict the median housing price, let's start with a linear regression model to get a baseline\n",
        "- We've already split our data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6fl2mEnW5yJ",
        "outputId": "30b61cc3-1945-4efe-edaf-19be4af1ee50"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate the model object\n",
        "linreg = LinearRegression(fit_intercept = True)\n",
        "\n",
        "# Fit the model to our dataset\n",
        "linreg.fit(X, y)\n",
        "\n",
        "# What coefficients do we come up with?\n",
        "print('Array of coefficients:\\n',linreg.coef_)\n",
        "print('Intercept:\\n', linreg.intercept_)\n",
        "\n",
        "# What is the R^2 value of the regression?\n",
        "print('R-squared coefficient: ', linreg.score(X, y))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array of coefficients:\n",
            " [[ 0.43669329  0.00943578 -0.10732204  0.64506569 -0.00000398 -0.00378654\n",
            "  -0.42131438 -0.43451375]]\n",
            "Intercept:\n",
            " [-36.94192021]\n",
            "R-squared coefficient:  0.606232685199805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4ufeNlYprJ"
      },
      "source": [
        "Some things to note here:\n",
        "- The decision to include an intercept in the regression is fully automated, so I need only specify the hyperparameter once (no need for leading ones in the design matrix)\n",
        "- All traditional elements of a regression fit are easily accessible\n",
        "- All we've done here is fit a model\n",
        "- To generate any kind of error or prediction, we'll use the `predict` method\n",
        "- We'll also need some kind of loss/error function to evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQsi5q3acY2",
        "outputId": "c2c28b6e-a6cd-4164-ae71-24d593a9de5a"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate predictions from training data\n",
        "y_preds = linreg.predict(X)\n",
        "\n",
        "# Generate Train and Test errors\n",
        "mse = mean_squared_error(y, y_preds, squared = False)\n",
        "\n",
        "\n",
        "# Setting squared=False gives Root MSE\n",
        "print('Train RMSE: ', mse)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE:  0.7241001216576387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MhJqFnPcGiX"
      },
      "source": [
        "- There are many other choices of evaluation metric beyond MSE (you can find many of them [here](https://scikit-learn.org/stable/modules/model_evaluation.html#)\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7chpquRcd9Jl"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __6. Logistic Regression__\n",
        "- Due to time constraints, I'm quickly going to pivot from regression (continuous response) to classification (in this case binary response)\n",
        "- For purposes of illustration, we'll take our outcome variable and transform it to a binary variable to illustrate logistic regression with scikit learn\n",
        "- Suppose we just want to know whether or not the house value is high. In this case, I define high as at or above the third quartile of all median housing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLrdFW-LgGBQ",
        "outputId": "3f2a192f-9ffe-40df-bc75-4410a0aee8ec"
      },
      "source": [
        "# Define new target threshold (third quartile)\n",
        "c = np.quantile(y, q = 0.75)\n",
        "\n",
        "# Generate new binary target\n",
        "t = (y > c).astype(float)\n",
        "print('Shape of new target', t.shape)\n",
        "print('Number of high-priced neighbourhoods: ',np.sum(t))\n",
        "print('First 5 labels:\\n',t[:5])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of new target (20640, 1)\n",
            "Number of high-priced neighbourhoods:  5160.0\n",
            "First 5 labels:\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AgHwLH1g1q7"
      },
      "source": [
        "Pretty straightforward. Note that this has now become a classification task, a\n",
        "we'll need a different model to account for the binary outcome.\n",
        "- Let's try logistic classification\n",
        "- We'll scale the features here for illustration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in9YPZhVhgEI",
        "outputId": "691b2c6b-9dea-42b8-e160-4b539ffb17ad"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate the transformer object\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# Fit to data (subtract mean and divide by SD for each column)\n",
        "X = std_scaler.fit_transform(X)\n",
        "\n",
        "# Instantiate the model object\n",
        "logit = LogisticRegression(fit_intercept = True, max_iter=1000, penalty = 'none')\n",
        "\n",
        "# Fit the model to the data\n",
        "logit.fit(X, t)\n",
        "\n",
        "# Examine the coefficients and intercept\n",
        "print('Logistic Regression Coefficients:\\n',logit.coef_)\n",
        "print('Intercept:\\n', logit.intercept_)\n",
        "\n",
        "# Get fitted values from training set\n",
        "t_train_preds_lr = logit.predict(X)\n",
        "\n",
        "# Print train and test errors\n",
        "train_acc = accuracy_score(t, t_train_preds_lr)\n",
        "\n",
        "print('Train Accuracy: ',train_acc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Coefficients:\n",
            " [[ 2.30114514  0.45812308 -0.65949543  0.84033213  0.09180904 -6.97973719\n",
            "  -3.68618503 -3.40001348]]\n",
            "Intercept:\n",
            " [-2.20978109]\n",
            "Train Accuracy:  0.8734496124031008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEYLjnlalznC"
      },
      "source": [
        "We can see that the performance (at least based on accuracy) is not terrible.  We will spend an entire lecture on evaluating model performance more carefully including various metrics for binary classifiers, train/test splits, etc in a couple of weeks. \n",
        "\n",
        "Note the hyperparameters I chose here:\n",
        "- I make sure to include the intercept\n",
        "- I also specify the number of iterations for the numerical solver (almost all model classes use a numerical solver like SGD instead of a closed form solution)\n",
        "- I used unpenalized regression (rather than ridge/lasso/etc which we haven't covered yet)\n",
        "- Can we do better?\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4LNrOv6Rtz"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# Conclusion\n",
        "- Today we looked at only a small demonstration of Scikit-Learn's coverage\n",
        "- They have many more preprocessing functions, evaluation metrics and models\n",
        "- They also allow easy stacking of each of these steps for when you want to deploy a full data science pipeline\n",
        "- You should now have everything you need to complete A3 (but you still have lots of time)\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "# __QUESTIONS?__"
      ]
    }
  ]
}